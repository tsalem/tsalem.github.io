<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    
    <title><b>Learning a Dynamic Map of Visual Appearance</b></title>
    
    <STYLE type=text/css media=all>
      #mainmedia {
        MARGIN-LEFT: auto; ;
        WIDTH: expression(document.body.clientWidth > 910? "910px": "auto" );
        MARGIN-RIGHT: auto;
        TEXT-ALIGN: left;
        max-width: 910px
      }
      h2
      {
         text-align: center
      }
		td{position:relative; z-index:0}
		.bottomimg{position:absolute; top:0; left:0; z-index:1}
		.topimg{position:absolute; top:0; left:0; z-index:2}
			 
    </STYLE>

    
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css">

    <!-- Custom styles for this template -->
    <link href="jumbotron.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy this line! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <!-- Main jumbotron for a primary marketing message or call to action -->
    <div id=mainmedia class="jumbotron">
      <div class="container">
        <h2>Learning a Dynamic Map of Visual Appearance</h2>
    <br>
    <br>
	<img src="img/architecture.png" alt="Geofaces" width="100%" height="100%">
     <h3><b>Abstract:</b></h3>
			The appearance of the world varies dramatically not only from place to place 
			but also from hour to hour and month to month. Every day billions of images 
			capture this complex relationship, many of which are associated with precise 
			time and location metadata. We propose to use these images to construct a global-scale, 
			dynamic map of visual appearance attributes. Such a map enables fine-grained understanding 
			of the expected appearance at any geographic location and time. Our approach integrates 
			dense overhead imagery with location and time metadata into a general framework capable 
			of mapping a wide variety of visual attributes. A key feature of our approach is that it 
			requires no manual data annotation. We demonstrate how this approach can support various applications, 
			including image-driven mapping, image geolocalization, and metadata verification. 
<p align="center">
</p>
    <div class="row" id="game-images" style="text-align:center;padding:0;margin:0">
        <div class="containere" id="image-containexr">
           <video width="852" height="480" controls>
                 <source src="paper-video.mp4" type="video/mp4">
            </video>
        </div>
    </div>
<hr>

<h3>People:</h3>

    <div class="row">
    <div class="col-md-5">
     <ul id=people>
		<li><a href="http://tsalem.github.io/">Tawfiq Salem</a>
		<li><a href="http://cs.uky.edu/~scott/">Scott Workman</a>
    <li><a href="http://cs.uky.edu/~jacobs/">Nathan Jacobs</a>
      </ul>
    </div>
    
  </div>


 <h3>Highlights:</h3>

        <ul id="highlights">
         
			<img src="img/visual_attributes.png" alt="Map of visual attributes over time" style="float:right" height="30%" width="40%">
			<h4><b><li> Visual Attribute Maps</b></h4>
			We propose a general convolutional neural network (CNN) architecture that visual attributes with
			overhead images, time, and geolocation. Several qualitative examples are shown in the figure on the right for different attributes over time.  
			
            <br><br><br><br><br><br><br><br><br><br><br><br>
			<img src="img/image_ret.png" alt="Image Retrieval" style="float:right" height="30%" width="40%">
            <h4><b><li>Image Retrieval</b></h4>
			In this qualitative application, we show how we can
			use our model to retrieve a set of ground-level images that
			would be likely to be observed at a given location and time.
			We start with an overhead image, specify a time of interest, and predict the visual attributes. We use the Combine
			distance defined in the previous section to find the closest
			ground-level images. In Figure 8, we show examples of
			images retrieved using this process
			
			<br><br><br>		
			<img src="img/image_loc.png" alt="Classifier Results" style="float:right" height="30%" width="40%">
			<h4><b><li>Iamge Localization</b></h4>
			we show qualitative localization results generated by our approach. For this experiment, 
			we used 488,224 overhead images from CVUSA as our reference database. The heatmap represents 
			the likelihood that an image was captured at a specific location, where red (blue) is more (less) likely.
			
			<br><br><br><br><br><br><br><br><br> 	
			<img src="img/metadata_verification.png" alt="Classifier Results" style="float:right" height="30%" width="40%">
			<h4><b><li>Metadata Verification</b></h4>
			We focus on verifying the time that an image, with known location, was captured. For a given ground-level image, 
			we compute the distance between the actual and predicted attributes resulting in a distance for each possible time
			The figure on the right shows heatmaps of these distances for one test examples, using our full model and the Combine distance
            
			</ul>
<br><br><br><br><br><br>
<h3><a name="papers"></a>Related Papers</h3>
  <ul>
  
  <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Salem_Learning_a_Dynamic_Map_of_Visual_Appearance_CVPR_2020_paper.pdf"> <p style="text-align:justify"><li>Learning a Dynamic Map of Visual Appearance
   </a>(Tawfiq Salem, Scott Workman, Nathan Jacobs),
  In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.</li></p> 
  <div>
	Bibtex:   
	<pre width="30">
	@inproceedings{salem2020dynamic,
	  author = {Salem, Tawfiq and Workman, Scott and Jacobs, Nathan},
	  booktitle = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
	  title = {{Learning a Dynamic Map of Visual Appearance}},
	  year = {2020},
	}
	</pre>
	</div>      
  </ul>
  <h3><a name="dataset"></a><b> Cross-View Time (CVT) Dataset:</b></h3>
  <img src="img/dataset.png" alt="Geofaces" width="100%" height="100%">
  <br><br> In our dataset, we have 305 011 ground-level images with the capture time and geolocation. 
  For each image, we have the orthorectified overhead image centered on the geographic location.
  
  <br><br><b> Please contact us by email to receive access to the database.</b>
  <br><br>
	



  </body>
</html>
